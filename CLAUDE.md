# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working
with code in this repository.

## Project Overview

Market Parser is a Python CLI and Gradio web interface for natural
language financial queries using the Polygon.io MCP server and OpenAI
GPT-5-nano via the OpenAI Agents SDK v0.2.9.

## Quick Start

### CLI Interface

```bash
# Using installed script (recommended)
uv run market-parser

# Standard Python entry point 
uv run src/main.py

# OR legacy method
uv run src/backend/cli.py

> Tesla stock analysis
KEY TAKEAWAYS
‚Ä¢ TSLA showing bullish momentum...
```

### Gradio Web Interface

```bash
# Start Gradio server
uv run python src/backend/gradio_app.py

# Access at http://127.0.0.1:8000
```


# üî¥ CRITICAL: MANDATORY TOOL USAGE to perform all task(s) - NEVER stop using tools - continue using them until tasks completion

üî¥ CRITICAL: You MUST use ALL available tools AS OFTEN AS NEEDED throughout the entire task execution. This is NOT a one-time checklist - you must continuously use tools throughout the process

## üî¥ MANDATORY: SYSTEMATIC TOOL USAGE ENFORCEMENT

**YOU MUST use Sequential-Thinking and Serena tools throughout ENTIRE implementation:**

- **START every phase** with Sequential-Thinking for systematic approach
- **Use Serena tools** for code analysis, symbol manipulation, pattern searches
- **Use Sequential-Thinking** repeatedly for complex reasoning and planning
- **Use Standard Read/Write/Edit** only when Serena doesn't support the specific operation
- **NEVER stop using advanced tools** until task completion

### Serena Tool Setup

Before using Serena tools for code analysis, get the initial instructions to understand optimal usage:

- **Primary Method (Preferred)**: Use `mcp__serena__initial_instructions` tool
- **Fallback Method**: If tool fails, use `mcp__serena__read_memory` with memory name: `serena_initial_instructions`

These instructions provide critical guidance on:

- Token-efficient code exploration using symbolic tools
- Proper file reading vs. full file scanning
- Symbol overview and targeted symbol reads
- Pattern searching for flexible searches

üî¥ REMEMBER: The tool list is your toolkit - use every tool as often as needed, in any order, throughout the entire task execution. Choose the right tool for the right operation

## üî¥ CRITICAL: MANDATORY TESTING CHECKPOINT - TWO-PHASE VALIDATION

**Testing is NOT optional - it is REQUIRED for task completion:**

### **Two-Phase Testing Workflow (MUST FOLLOW):**

1. **Code Implementation** ‚Üí Create/update code
2. **Test Suite Update** ‚Üí Create/update test files
3. **üî¥ PHASE 1 (MANDATORY)** ‚Üí RUN the test suite to generate responses
4. **üî¥ PHASE 2 (MANDATORY)** ‚Üí MANUALLY VERIFY each response for correctness
5. **Documentation** ‚Üí Update docs with test results

### **Phase 1: Automated Response Generation**

‚úÖ **MUST DO:**

- Execute test suite: `chmod +x test_cli_regression.sh && ./test_cli_regression.sh`
- Script generates all 39 test responses
- Script reports "X/39 COMPLETED" (responses received)
- **LIMITATION**: Script CANNOT validate response correctness
- Show Phase 1 results to user (completion counts, response times)
- Provide test report file path

### **Phase 2: MANDATORY Manual Verification of ALL 39 Tests (NO SHORTCUTS)**

üî¥ **CRITICAL: Grep commands are INSUFFICIENT and will miss failures. You MUST manually review EACH test response.**

**Why Grep Fails:**
- ‚ùå Misses duplicate/unnecessary tool calls (agent calling same tool twice)
- ‚ùå Misses wrong tool selection (agent calling wrong API for data)
- ‚ùå Misses data inconsistencies (cross-ticker contamination, wrong data returned)
- ‚ùå Only catches explicit error messages, not logic errors

**MANDATORY Process for EACH of the 39 Tests:**

#### **Step 1: Read Test Response Using Read Tool**
- Use `Read` tool to read the test log file section for each test
- Read lines corresponding to that test's Agent Response, Tools Used, and Performance Metrics
- **NO scripts, NO grep shortcuts - READ each test manually**

#### **Step 2: Apply 4-Point Verification Criteria**

For EACH test, you MUST verify ALL 4 criteria:

1. ‚úÖ **Does the response address the query?**
   - Does the agent's response directly answer the test prompt?
   - Is the response relevant to the ticker(s) mentioned?
   - Is the response complete (not truncated)?

2. ‚úÖ **Were the RIGHT tools called (not duplicate/unnecessary calls)?**
   - **Check conversation context**: If a previous test already retrieved data, agent should NOT call the same tool again
   - Example FAIL: Test 10 calls `get_ta_indicators()`, Test 12 should NOT call it again
   - Are the tools appropriate for the query (Tradier for quotes, Polygon for TA)?
   - Are there any redundant API calls?

3. ‚úÖ **Is the data correct?**
   - Correct ticker symbols used ($SPY, $NVDA, $WDC, $AMD, $SOUN)
   - Data formatting matches expected format (OHLC, tables, etc.)
   - No hallucinated data or made-up values
   - No cross-ticker contamination (NVDA query shouldn't return SPY data)
   - Options chains show Bid/Ask columns (NOT midpoint)

4. ‚úÖ **Are there any errors?**
   - No error messages in response
   - No "data unavailable" messages
   - No RuntimeWarnings
   - No API errors

#### **Step 3: Document Each Test Result**

Create a table documenting ALL 39 tests:

| Test # | Test Name | Status | Issue (if failed) | Failure Type |
|--------|-----------|--------|-------------------|--------------|
| 1 | Market_Status | ‚ùå FAIL | timezone import error | Code Error |
| 2 | SPY_Price | ‚úÖ PASS | - | - |
| 10 | SPY_TA_Indicators | ‚úÖ PASS | - | - |
| 12 | SPY_Full_TA | ‚ùå FAIL | Duplicate call to get_ta_indicators() | Logic Error (Duplicate Tool Call) |
| ... | ... | ... | ... | ... |

**Failure Types:**
- Code Error: Syntax/runtime errors, import errors
- Logic Error (Duplicate Tool Call): Agent made unnecessary redundant API calls
- Logic Error (Wrong Tool): Agent called wrong tool for the query
- Data Error: Wrong data returned, cross-ticker contamination
- Response Error: Incomplete response, doesn't address query

#### **Step 4: Final Checkpoint Questions**

Answer ALL checkpoint questions with evidence:

1. ‚úÖ Did you READ all 39 test responses manually using the Read tool? **YES/NO**
2. ‚úÖ Did you apply all 4 verification criteria to EACH test? **YES/NO**
3. ‚úÖ How many tests PASSED all 4 criteria? **X/39 PASSED**
4. ‚úÖ How many tests FAILED (any criterion)? **X/39 FAILED**
5. ‚úÖ Did you document ALL failures with test #, issue, and failure type? **YES/NO + TABLE**

**üî¥ CANNOT MARK TASK COMPLETE WITHOUT:**
- Reading all 39 test responses manually (using Read tool, NOT grep)
- Applying all 4 verification criteria to each test
- Documenting ALL 39 tests in a results table
- Providing failure count and failure details table
- Answering all 5 checkpoint questions with evidence

‚ùå **NEVER DO:**

- Skip Phase 1 test execution
- Skip Phase 2 manual verification
- Claim completion without test results AND manual verification
- Mark task "done" without Phase 2 evidence
- Proceed to documentation without running both phases

### **Enforcement Rules:**

üî¥ **Code without Phase 1 execution = Code NOT implemented**
üî¥ **Phase 1 without Phase 2 verification = Testing INCOMPLETE**
üî¥ **No manual verification = Task INCOMPLETE**
üî¥ **Phase 2 verification is PROOF of correctness**
üî¥ **Both phases must complete BEFORE documentation updates**

### **Key Insight:**

The script saying "39/39 COMPLETED" means "39 responses received" NOT "39 tests passed validation".

Only after Phase 2 manual verification can you claim tests passed.

### **Pattern Recognition:**

**WRONG (What NOT to do):**

```text
1. Create 5 new tools ‚úÖ
2. Update test suite file ‚úÖ
3. RUN Phase 1: ./test_cli_regression.sh ‚úÖ
4. Show results: 39/39 COMPLETED ‚úÖ
5. ‚ùå Run grep commands to "verify" results (INSUFFICIENT - misses duplicate tool calls!)
6. Update documentation ‚úÖ
7. Mark task complete ‚ùå (NEVER manually reviewed all 39 test responses!)
```

**CORRECT (What TO do):**

```text
1. Create 5 new tools ‚úÖ
2. Update test suite file ‚úÖ
3. RUN Phase 1: chmod +x test_cli_regression.sh && ./test_cli_regression.sh ‚úÖ
4. Show Phase 1 results: 39/39 COMPLETED ‚úÖ
5. PERFORM Phase 2: Use Read tool to manually read EACH of the 39 test responses ‚úÖ
6. Apply 4-point criteria to EACH test:
   - Does response address query? ‚úÖ
   - Were RIGHT tools called (no duplicates)? ‚úÖ
   - Is data correct? ‚úÖ
   - Are there any errors? ‚úÖ
7. Document ALL 39 tests in results table (PASS/FAIL with reasons) ‚úÖ
8. Answer checkpoint questions with evidence ‚úÖ
9. Provide test report path ‚úÖ
10. Update documentation with test results ‚úÖ
11. Mark task complete ‚úÖ
```

### **When to Run Tests:**

- After creating new tools/functions
- After modifying existing code
- After updating AI agent instructions
- After changing test suite
- Before updating documentation
- Before claiming task completion

**Remember: If you haven't RUN Phase 1, PERFORMED Phase 2, and SHOWN both results, the task is NOT complete.**

## üî¥ CRITICAL: PROPER ATOMIC COMMIT WORKFLOW

### MANDATORY: Stage ONLY Immediately Before Commit

#### The Fatal Mistake: Early Staging

**NEVER stage files early during development. Staging is the LAST step before committing.**

**What happens when you stage too early:**

1. ‚è∞ **Time T1**: You run `git add` (files staged)
   - Staging area = snapshot at T1
2. ‚è∞ **Time T2-T5**: You continue working
   - Update config files
   - Run tests (generates test reports)
   - Update documentation
3. ‚è∞ **Time T6**: You run `git commit`
   - **Only commits the T1 snapshot**
   - **All work after T1 is MISSING**

**Result: Incomplete, broken atomic commits** ‚ùå

### Correct Atomic Commit Workflow

**Follow this workflow EXACTLY:**

1. **DO ALL WORK FIRST** (DO NOT stage anything yet):
   - ‚úÖ Complete ALL code changes
   - ‚úÖ Run ALL tests and generate test reports
   - ‚úÖ Update ALL documentation (CLAUDE.md, tech_stack.md, etc.)
   - ‚úÖ Update ALL config files (.claude/settings.local.json, etc.)
   - ‚úÖ Update ALL Serena memories
   - ‚úÖ Update ALL task plans
   - ‚ö†Ô∏è **DO NOT RUN `git add` YET**

2. **VERIFY EVERYTHING IS COMPLETE**:

   ```bash
   git status  # Review ALL changed/new files
   git diff    # Review ALL changes
   ```

3. **STAGE EVERYTHING AT ONCE**:

   ```bash
   git add -A  # Stage ALL files in ONE command
   ```

   - ‚ö†Ô∏è This is the FIRST time you run `git add`

4. **VERIFY STAGING IMMEDIATELY**:

   ```bash
   git status  # Verify ALL files staged, NOTHING unstaged
   ```

5. **COMMIT IMMEDIATELY** (within 60 seconds):

   ```bash
   git commit -m "message"
   ```

6. **PUSH IMMEDIATELY**:

   ```bash
   git push
   ```

### What Belongs in an Atomic Commit

**ALL of these must be included together:**

- ‚úÖ Code changes (backend + Gradio UI)
- ‚úÖ Test reports (evidence of passing tests)
- ‚úÖ Documentation updates (CLAUDE.md, README.md, etc.)
- ‚úÖ Memory updates (.serena/memories/)
- ‚úÖ Config changes (.claude/settings.local.json, etc.)
- ‚úÖ Task plan updates (TODO_task_plan.md, etc.)

### ‚ùå NEVER DO THIS

- ‚ùå Stage files early during development
- ‚ùå Stage files "as you go"
- ‚ùå Run `git add` before ALL work is complete
- ‚ùå Delay between `git add` and `git commit`
- ‚ùå Commit without test reports
- ‚ùå Commit without documentation updates

**Reference:** See `.serena/memories/git_commit_workflow.md` for complete details.

**Enforcement:** Incomplete commits will be reverted and reworked.



## Application Startup

### Simple Command Startup

**Prerequisites:** uv, API keys in .env

**CLI Interface (recommended for automation/scripting):**
```bash
# Standard Python entry point (recommended)
uv run main.py

# Using installed console script
uv run market-parser

# Legacy method (still supported)
uv run src/backend/cli.py
```

**Gradio Web UI (recommended for interactive analysis):**
```bash
# Hot reload mode (recommended for development)
# Auto-reloads on file save, 2x-10x less CPU than standard server auto-reload
uv run gradio src/backend/gradio_app.py

# Using installed console script
uv run market-parser-gradio

# Production mode (no hot reload)
uv run python src/backend/gradio_app.py

# Access at http://127.0.0.1:8000
# PWA enabled: Install from browser address bar (Chrome/Edge)
```

## Features

### Natural Language Financial Queries

Ask questions like:

- `Tesla stock price analysis`
- `AAPL volume trends this week`
- `Show me MSFT support and resistance levels`

### Multiple Interfaces

- **Gradio Web Interface** - Python ChatInterface for financial analysis (port 8000)
- **Enhanced CLI** - Terminal interface with rich formatting

## Example Usage

### Gradio Web Interface

1. Open <http://127.0.0.1:8000>
2. Select an example or type your financial query
3. Get streaming responses with financial data and analysis
4. Examples included: Stock price queries, technical analysis, options chains, stock comparisons

**Example Response with Performance Metrics:**

```text
Market Status: CLOSED
After-hours: NO
Early-hours: NO
Exchanges: NASDAQ closed, NYSE closed, OTC closed
Server Time (UTC): 2025-10-18 01:50:12
Date: 2025-10-18

Performance Metrics:
   Response Time: 5.135s
   Tokens Used: 21,701 (Input: 21,402, Output: 299)
   Model: gpt-5-nano
```

## Architecture

- **Core**: CLI with OpenAI Agents SDK v0.2.9 and Direct Polygon/Tradier API integration
- **Web UI**: Gradio 5.49.1+ ChatInterface (port 8000) - wraps CLI core
- **Testing**: CLI regression test suite (test_cli_regression.sh - 39 tests)
- **Pattern**: CLI = core business logic, Gradio = thin wrapper (zero duplication)

## Development

### Available Commands

```bash
# CLI Interface (all methods work)
uv run main.py                    # Standard Python entry point (recommended)
uv run market-parser              # Console script
uv run src/backend/cli.py         # Legacy method

# Gradio Web UI (all methods work)
uv run gradio src/backend/gradio_app.py   # Hot reload mode (recommended for dev)
uv run market-parser-gradio               # Console script
uv run python src/backend/gradio_app.py   # Production mode (no hot reload)

# Testing
chmod +x test_cli_regression.sh && ./test_cli_regression.sh  # 39-test suite

# Code quality
npm run lint              # Python linting
npm run lint:fix          # Auto-fix with black + isort
```

### Project Structure

```text
src/
‚îú‚îÄ‚îÄ backend/              # All application code
‚îÇ   ‚îú‚îÄ‚îÄ cli.py           # CLI interface (CORE BUSINESS LOGIC)
‚îÇ   ‚îú‚îÄ‚îÄ gradio_app.py    # Gradio web UI (wraps CLI core)
‚îÇ   ‚îú‚îÄ‚îÄ tools/           # AI agent tools (Polygon, Tradier)
‚îÇ   ‚îî‚îÄ‚îÄ services/        # Agent service layer
config/                  # Centralized configuration
‚îÇ   ‚îî‚îÄ‚îÄ app.config.json # Non-sensitive settings
```

## Troubleshooting

### Common Issues

**CLI not starting:**

```bash
# Check .env file has API keys
cat .env | grep API_KEY

# Verify dependencies
uv sync
```

**Gradio UI not loading:**

```bash
# Check port availability
netstat -tlnp | grep :8000

# Kill existing Gradio processes
pkill -f gradio_app

# Restart
uv run python src/backend/gradio_app.py
```

**API key issues:**

- Ensure both `POLYGON_API_KEY` and `OPENAI_API_KEY` are set in `.env`
- Verify API keys are valid and have sufficient credits


## claude --dangerously-skip-permissions

## uvx --from git+https://github.com/oraios/serena serena project index

IMPORTANT: this context may or may not be relevant to your tasks. You should not respond to this context unless it is highly relevant to your task.

## Last Completed Task Summary

<!-- LAST_COMPLETED_TASK_START -->
[RESPONSE_FORMATTING_TRANSPARENCY] Add Response Formatting Transparency to Agent Responses

**Summary:** Successfully added "Response Formatting" transparency metadata to AI Agent responses to document whether and why tool outputs are reformatted. Feature helps debug agent behavior by making formatting decisions explicit. All 37 regression tests pass with 100% compliance - every response includes proper [YES]/[NO]/[N/A] formatting transparency.

**Implementation (Phases 1-5 - COMPLETED):**
- ‚úÖ **Research:** OpenAI Agents SDK pattern analysis, identified existing "Tools Used" pattern as baseline
- ‚úÖ **Planning:** Designed [YES/NO/N/A] transparency format with required explanations, 8-phase implementation plan
- ‚úÖ **Code Update:** Added 35-line Response Formatting Instruction block to agent_service.py (+200 tokens)
  - Format: [YES] when agent reformats (MUST explain WHAT and WHY)
  - Format: [NO] when agent preserves output (MUST explain WHY)
  - Format: [N/A] when no tool calls (using existing data)
  - Positioned after "Tools Used" section for consistent metadata pattern
- ‚úÖ **Manual Testing:** 5 manual CLI prompts - all showed Response Formatting section working correctly
- ‚úÖ **Regression Testing:** Phase 1 executed 37/37 tests, Phase 2 manually verified ALL 37 responses

**Test Results (37/37 PASSED):**
- ‚úÖ **100% Feature Compliance:** All 37 tests include "Response Formatting:" section
- ‚úÖ **Distribution:** 0 [YES], 4 [NO], 33 [N/A] - agent correctly preserves pre-formatted tool outputs
- ‚úÖ **Test Coverage:** SPY (tests 1-15), NVDA (16-29), Multi-ticker (30-37) - all categories validated
- ‚úÖ **Performance:** Min 6.060s, Max 29.663s, Avg 11.43s - no regression in response times
- ‚úÖ **Manual Verification:** Applied 4-point criteria to each test (response accuracy, tool selection, data correctness, error checking)

**Files Modified:**
- ‚úÖ src/backend/services/agent_service.py (agent instructions updated with Response Formatting block)
- ‚úÖ test-reports/test_cli_regression_loop1_2025-10-29_20-16.log (37 test responses with transparency metadata)

**Code Quality Summary:**
- ‚úÖ Instruction-only change, no code logic modifications
- ‚úÖ Syntax verified: 10,508 characters, loads successfully
- ‚úÖ No broken functionality or tool regressions
- ‚úÖ All existing RULE references preserved and working correctly
- ‚úÖ Agent self-reflection working as intended

**Risk Assessment:** VERY LOW ‚úÖ
- ‚úÖ Instruction-only change (no code modifications)
- ‚úÖ All 37 regression tests pass with 100% success rate
- ‚úÖ All tests manually verified with 4-point criteria
- ‚úÖ Zero functionality regressions detected
- ‚úÖ Improves debugging without changing agent behavior

**Git Commit Strategy:**
- Atomic commit includes code update + test reports + documentation updates
- Single comprehensive commit message documenting feature addition
- Test evidence included (test report showing 37/37 with 100% Response Formatting compliance)
<!-- LAST_COMPLETED_TASK_END -->
