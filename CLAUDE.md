# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working
with code in this repository.

## Project Overview

Market Parser is a Python CLI and React web application for natural
language financial queries using the Polygon.io MCP server and OpenAI
GPT-5-nano via the OpenAI Agents SDK v0.2.9.

## Last Completed Task Summary

<!-- LAST_COMPLETED_TASK_START -->
[TESTING] Add automated loop support to CLI_test_regression.sh

- Enhanced CLI_test_regression.sh with automated loop testing (1-10 iterations)
- Deleted legacy test_16_prompts_persistent_session.sh
- **Loop Parameter Support**: `./CLI_test_regression.sh [LOOP_COUNT]` (default: 1, max: 10)
- **Aggregate Statistics**: Min/max/avg response times across all loop iterations
- **Individual Reports**: Separate timestamped report per loop iteration

**Loop Enhancement Features:**

1. **Parameter Validation**: Enforces 1-10 loop range with clear error messages
2. **Per-Loop Tracking**: Each loop runs all 27 tests in single persistent session
3. **Aggregate Analysis**: Overall statistics calculated across all loops
4. **Report Generation**: Unique timestamped reports per loop iteration
5. **Loop Progress**: Visual indicators showing current loop (e.g., "LOOP 2/3")

**Test Results (1x and 3x Loop Validation):**

**1-Loop Test (Baseline):**

- Total Tests: 27/27 PASSED
- Avg Response Time: 6.44s
- Performance: EXCELLENT
- Report: cli_regression_test_loop1_20251005_171905.txt

**3-Loop Test (Validation):**

- Total Loops: 3/3 PASSED (100% success rate)
- Loop 1 Avg: 9.19s (27/27 PASS)
- Loop 2 Avg: 9.06s (27/27 PASS)
- Loop 3 Avg: 9.38s (27/27 PASS)
- Overall Avg: 9.21s (EXCELLENT performance)
- Reports Generated: 3 individual loop reports

**Documentation Updates:**
‚úÖ Updated tech_stack.md with loop automation documentation
‚úÖ Updated Code Quality Checklist: `./CLI_test_regression.sh [LOOP_COUNT]`
‚úÖ Documented loop usage examples and aggregate statistics

**Files Changed:**

- ‚ùå Deleted: test_16_prompts_persistent_session.sh
- ‚úÖ Enhanced: CLI_test_regression.sh (added loop support with 1-10 iterations)
- ‚úÖ Updated: .serena/memories/tech_stack.md
- ‚úÖ Test Reports: 4 total (1x baseline + 3x loops validation)
<!-- LAST_COMPLETED_TASK_END -->

## üî¥ CRITICAL: MANDATORY TOOL USAGE to perform all task(s) - NEVER stop using tools - continue using them until tasks completion

üî¥ CRITICAL: You MUST use ALL available tools AS OFTEN AS NEEDED throughout the entire task execution. This is NOT a one-time checklist - you must continuously use tools throughout the process.

üî¥ REMEMBER: The tool list is your toolkit - use every tool as often as needed, in any order, throughout the entire task execution. Choose the right tool for the right operation

TOOL USAGE REQUIREMENTS:

- Use tools in ANY ORDER as needed for the specific task
- Use the SAME tool MULTIPLE TIMES if needed
- NEVER treat tool lists as a rigid sequence
- ALWAYS use tools when they would be helpful, even if you've used them before
- Use tools for investigation, analysis, verification, and implementation at every step

MANDATORY TOOL USAGE PATTERNS:

1. START with Sequential-Thinking for task analysis, Investigation, Planning, Scoping, Researching, Complex problem analysis (max 8 thoughts)
2. Use Serena Tools for code analysis, symbol manipulation, pattern search with context, and memory management for complex financial algorithm development and refactoring; Use standard Read/Write/Edit for simple file content modifications
3. REPEAT any tool as needed throughout the process
4. üî¥ NEVER stop using tools - continue using them until task completion

VIOLATION PENALTIES:

- If you use tools only once and stop, you're failing
- If you follow a rigid order instead of using tools as needed, you're failing
- If you don't use tools throughout the entire process, you're failing
- If you use wrong tool for the operation (e.g., Standard for batch operations), you're failing

SUCCESS CRITERIA:

- Tools used multiple times throughout the task
- Tools used in different orders based on need
- Continuous tool usage from start to finish
- Correct tool selection based on operation type
- No rigid sequencing - only logical tool usage based on task requirements

üî¥ REMEMBER: The tool list is your toolkit - use every tool as often as needed, in any order, throughout the entire task execution. Choose the right tool for the right operation

## üî¥ CRITICAL: MANDATORY TESTING CHECKPOINT

**Testing is NOT optional - it is REQUIRED for task completion:**

### **Testing Workflow (MUST FOLLOW):**

1. **Code Implementation** ‚Üí Create/update code
2. **Test Suite Update** ‚Üí Create/update test files
3. **üî¥ TEST EXECUTION (MANDATORY)** ‚Üí RUN the test suite
4. **Test Verification** ‚Üí Verify 100% pass rate
5. **Documentation** ‚Üí Update docs with test results

### **Test Execution Requirements:**

‚úÖ **MUST DO:**

- Execute test suite (e.g., `./test_16_prompts_persistent_session.sh`)
- Show test results to user (pass/fail counts, response times)
- Verify 100% success rate
- Provide test report file path
- Fix any failures and re-test

‚ùå **NEVER DO:**

- Skip test execution
- Claim completion without test results
- Mark task "done" without test evidence
- Proceed to documentation without running tests

### **Enforcement Rules:**

üî¥ **Code without test execution = Code NOT implemented**
üî¥ **No test results = Task INCOMPLETE**
üî¥ **Test results are PROOF of implementation**
üî¥ **Tests must run BEFORE documentation updates**

### **Pattern Recognition:**

**WRONG (What NOT to do):**

```text
1. Create 5 new tools ‚úÖ
2. Update test suite file ‚úÖ
3. Update documentation ‚úÖ
4. Mark task complete ‚ùå (NEVER ran tests!)
```

**CORRECT (What TO do):**

```text
1. Create 5 new tools ‚úÖ
2. Update test suite file ‚úÖ
3. RUN test suite: ./test_16_prompts_persistent_session.sh ‚úÖ
4. Show results: 16/16 PASS, 100% success ‚úÖ
5. Provide test report path ‚úÖ
6. Update documentation with test results ‚úÖ
7. Mark task complete ‚úÖ
```

### **When to Run Tests:**

- After creating new tools/functions
- After modifying existing code
- After updating AI agent instructions
- After changing test suite
- Before updating documentation
- Before claiming task completion

**Remember: If you haven't RUN the tests and SHOWN the results, the task is NOT complete.**

## Quick Start

### CLI Interface

```bash
uv run src/backend/main.py

> Tesla stock analysis
KEY TAKEAWAYS
‚Ä¢ TSLA showing bullish momentum...
```

**One-Click Application Startup (Recommended):**

The startup scripts automatically START all development servers BUT **DOES
NOT OPEN THE APP IN BROWSER AUTOMATICALLY**.

```bash
# Option 1: XTerm startup script (RECOMMENDED - WORKING)
./start-app-xterm.sh

# Option 2: Main startup script (NOW WORKING - FIXED)
./start-app.sh  # ‚úÖ WORKING: Script now exits cleanly with timeout
```

**Prerequisites:** uv, Node.js 18+, API keys in .env

## Script Variants

### start-app.sh (NOW WORKING - FIXED)

- **Status**: ‚úÖ WORKING - Script now exits cleanly with timeout mechanism
- **Features**: 30-second timeout fallback to prevent hanging
- **Environment Support**: Works in both X11 and WSL2/headless environments
- **Background Mode**: Uses background processes in WSL2, terminal windows in X11
- **Logging**: Writes server logs to backend.log and frontend.log in WSL2 mode

## What the Scripts Do

### ‚è∞ Timeout Mechanism

Both scripts now include a **30-second timeout fallback** to prevent hanging:

- **Normal Operation**: Scripts typically complete in 10-15 seconds
- **Safety Net**: 30-second timeout ensures scripts never hang indefinitely
- **AI Agent Friendly**: Prevents AI agents from getting stuck waiting for script completion
- **Graceful Exit**: Scripts exit cleanly after server verification or timeout

### üîÑ Server Cleanup

- Kills existing development servers (uvicorn, vite)
- **Preserves MCP servers** - does not interfere with MCP processes
- Waits for processes to terminate gracefully

### üöÄ Server Startup

- **Backend**: Starts FastAPI server on `http://127.0.0.1:8000`
- **Frontend**: Starts Vite dev server on `http://127.0.0.1:3000`
- Opens each server in a separate terminal window for easy monitoring
- Uses consistent hard-coded ports (no dynamic allocation)

### ‚úÖ Health Verification

- Performs health checks on both servers
- Retries up to 10 times with 2-second intervals
- Verifies backend `/health` endpoint responds
- Verifies frontend serves content properly

### üåê Browser Launch

- **NOTIFIES USER TO LAUNCH BROWSER TO START THE APP WHEN SERVERS ARE READY**

**Access:** <http://127.0.0.1:3000> (React app) or <http://127.0.0.1:8000> (API docs)

## Features

### ‚ö° High-Performance UI

- **Lightning Fast Loading**: 85%+ improvement in Core Web Vitals
- **Optimized Performance**: 256ms First Contentful Paint (FCP)
- **Smooth Interactions**: All UI interactions are instant and responsive
- **Memory Efficient**: Optimized memory usage with 13.8MB heap size
- **Accessibility First**: Full WCAG 2.1 AA compliance

### Natural Language Financial Queries

Ask questions like:

- `Tesla stock price analysis`
- `AAPL volume trends this week`
- `Show me MSFT support and resistance levels`

### Multiple Interfaces

- **React Web App** - Modern responsive interface with real-time chat
- **Enhanced CLI** - Terminal interface with rich formatting
- **API Endpoints** - RESTful API for integration

## Example Usage

### Web Interface

1. Open <http://127.0.0.1:3000>
2. Type your financial query
3. Get instant structured responses with sentiment analysis

## Architecture

- **Backend**: FastAPI with OpenAI Agents SDK v0.2.9 and Polygon.io MCP integration v0.4.1
- **Frontend**: React 18.2+ with Vite 5.2+ and TypeScript
- **Testing**: Playwright E2E test suite
- **Deployment**: Fixed ports (8000/3000/5500) with one-click startup

## Development

### Available Commands

```bash
# Application startup
npm run start:app          # One-click startup
npm run frontend:dev       # Frontend development
npm run build             # Production build

# Testing with Playwright MCP Tools only - see `/tests/playwright/mcp_test_script_basic.md`

# Code quality
npm run lint              # All linting
npm run type-check        # TypeScript validation
```

### Project Structure

```text
src/
‚îú‚îÄ‚îÄ backend/              # FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ main.py          # Main application
‚îÇ   ‚îú‚îÄ‚îÄ api_models.py    # API schemas
‚îÇ   ‚îî‚îÄ‚îÄ prompt_templates.py # Analysis templates
‚îú‚îÄ‚îÄ frontend/            # React frontend
‚îÇ   ‚îú‚îÄ‚îÄ components/      # React components
‚îÇ   ‚îú‚îÄ‚îÄ hooks/          # Custom hooks
‚îÇ   ‚îî‚îÄ‚îÄ config/         # Configuration loader
config/                  # Centralized configuration
‚îÇ   ‚îî‚îÄ‚îÄ app.config.json # Non-sensitive settings
tests/playwright/        # E2E test suite
```

## Troubleshooting

### Common Issues

**Backend not starting:**

```bash
# Check .env file has API keys
cat .env | grep API_KEY

# Verify dependencies
uv install
```

**Frontend connection errors:**

```bash
# Verify backend is running
curl http://127.0.0.1:8000/health

# Check ports are available
netstat -tlnp | grep :8000
```

**API key issues:**

- Ensure both `POLYGON_API_KEY` and `OPENAI_API_KEY` are set in `.env`
- Verify API keys are valid and have sufficient credits

## Disclaimer

**Warning:** This application uses AI and large language models.
Outputs may contain inaccuracies and should not be treated as financial
advice. Always verify information independently before making financial
decisions. Use for informational purposes only.

## License

This project is licensed under the [MIT License](LICENSE).
