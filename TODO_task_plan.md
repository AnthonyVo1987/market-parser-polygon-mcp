# TODO Task Plan: Response Formatting Transparency Implementation

**Status: ‚úÖ COMPLETED (2025-10-29)** - All 8 phases executed successfully, feature validated and deployed

## Overview

Implement "Response Formatting" transparency metadata in AI Agent responses to help debug formatting issues. This feature adds a new metadata section after "Tools Used" that documents whether and why the agent reformatted tool outputs.

**Estimated Duration:** 2-3 hours (Actual: 2h 45m)
**Complexity:** LOW (instruction-only change)
**Risk Level:** VERY LOW (no code logic changes)

---

## Phase 1: Research ‚úÖ COMPLETED

- ‚úÖ Research OpenAI Agents SDK response patterns
- ‚úÖ Analyze current agent_service.py structure
- ‚úÖ Design Response Formatting metadata format
- ‚úÖ Create research_task_plan.md
- ‚úÖ Create TODO_task_plan.md

**Status:** COMPLETE
**Duration:** 20 minutes
**Artifacts:** research_task_plan.md, TODO_task_plan.md

---

## Phase 2: Planning (CURRENT PHASE - AWAITING USER APPROVAL)

### Task 2.1: Present Plans to User

**Action:** Notify user that research and planning are complete
**Deliverables:**
- research_task_plan.md (comprehensive research findings)
- TODO_task_plan.md (this file - detailed implementation steps)

**User Review Checklist:**
1. Does the proposed Response Formatting format meet requirements? ‚úì
2. Is the instruction text clear and unambiguous? ‚úì
3. Is the implementation approach sound? ‚úì
4. Are there any concerns or modifications needed? ‚úì

**GATE:** Implementation cannot proceed until user approves plans

---

## Phase 3: User Plan Approval (GATE)

**Status:** PENDING USER APPROVAL

**User Decision Options:**
1. ‚úÖ **APPROVE** - Proceed to Phase 4 (Implementation)
2. üîÑ **MODIFY** - Request changes to plan, return to Phase 2
3. ‚ùå **REJECT** - Cancel implementation, revise approach

**Wait for explicit user approval before proceeding to Phase 4**

---

## Phase 4: Implementation (BLOCKED UNTIL APPROVAL)

### Task 4.1: Update Agent Instructions

**File:** `src/backend/services/agent_service.py`
**Function:** `get_enhanced_agent_instructions()`
**Action:** Add new instruction block using Serena tools

**Serena Tool Workflow:**

1. **Read current function using Serena:**
   ```python
   mcp__serena__find_symbol(
       name_path="get_enhanced_agent_instructions",
       relative_path="src/backend/services/agent_service.py",
       include_body=True
   )
   ```

2. **Identify insertion point:**
   - Current: Ends with TOOL CALL TRANSPARENCY REQUIREMENT example
   - Target: Add new section immediately after (before closing triple-quote)
   - Line: Approximately line 256-258 (after Tools Used example)

3. **Prepare new instruction text:**
   ```python
   NEW_INSTRUCTION_BLOCK = """

   üìã RESPONSE FORMATTING TRANSPARENCY REQUIREMENT:

   At the END of EVERY response, IMMEDIATELY AFTER the "Tools Used" section, you MUST include a "Response Formatting" section that documents your formatting decisions:

   **Response Formatting:**
   [YES/NO/N/A] Explanation of formatting decision

   FORMAT OPTIONS:

   1. [YES] - You reformatted or restructured tool output
      - MUST describe WHAT changes you made (e.g., "Converted raw data into markdown table")
      - MUST explain WHY you made those changes (e.g., "for better readability")
      - Example: "[YES] Converted raw options chain data into structured markdown table with Bid/Ask columns for better readability"

   2. [NO] - You preserved tool output exactly as returned
      - MUST explain WHY no formatting was needed (e.g., "Tool output already in optimal format")
      - Example: "[NO] Tool returned pre-formatted markdown table, preserved as-is per RULE #9"

   3. [N/A] - No tool calls were made (using existing data)
      - Use when no new tool data to format
      - Example: "[N/A] No tool calls made, response based on existing data from chat history"

   CRITICAL RULES:
   - üî¥ ALWAYS include this section in EVERY response (no exceptions)
   - üî¥ Place AFTER "Tools Used" section (not before)
   - üî¥ Be specific about WHAT formatting changes were made
   - üî¥ NEVER say just "[YES]" or "[NO]" without explanation

   Example complete response ending:
   ---
   **Tools Used:**
   - `get_stock_quote(ticker='SPY')` - Single ticker request per RULE #1

   **Response Formatting:**
   [NO] Tool returned clean price data that was already structured, no reformatting applied
   """
   ```

4. **Insert using Serena insert_after_symbol:**
   ```python
   mcp__serena__insert_after_symbol(
       name_path="get_enhanced_agent_instructions",
       relative_path="src/backend/services/agent_service.py",
       body=NEW_INSTRUCTION_BLOCK
   )
   ```

   **WAIT** - This approach won't work because we need to insert WITHIN the function's return string, not after the function itself.

   **CORRECT APPROACH:** Use Edit tool to modify the instruction string

5. **Use Edit tool to insert new block:**
   - Find the closing triple-quote of the instruction string
   - Insert new instruction block before the closing triple-quote
   - Preserve all existing formatting and indentation

**Expected Changes:**
- Lines added: ~35 lines
- Tokens added: ~200 tokens
- Total instruction size: ~2200 tokens (up from ~2000)

**Success Criteria:**
- ‚úÖ New instruction block appears in agent instructions
- ‚úÖ No syntax errors in Python function
- ‚úÖ Triple-quote string properly closed
- ‚úÖ Indentation consistent with existing code

---

### Task 4.2: Verify Agent Service Syntax

**Action:** Validate that agent_service.py has no syntax errors

**Commands:**
```bash
# Check Python syntax
uv run python -m py_compile src/backend/services/agent_service.py

# Verify agent instructions load correctly
uv run python -c "from src.backend.services.agent_service import get_enhanced_agent_instructions; print('SUCCESS: Instructions loaded')"
```

**Success Criteria:**
- ‚úÖ No syntax errors reported
- ‚úÖ Function imports successfully
- ‚úÖ Instructions return as string

**Troubleshooting:**
- If syntax error: Review triple-quote closure and indentation
- If import error: Check function definition and dependencies

---

## Phase 5: Manual Targeted Testing

### Task 5.1: Manual Test Prompts

**Goal:** Verify new Response Formatting section appears in responses

**Test Command:**
```bash
uv run market-parser
```

**Test Prompts (5 minimum):**

1. **Test 1: Simple Stock Quote**
   ```
   Prompt: "Get quote for SPY"
   Expected Tools Used: get_stock_quote(ticker='SPY')
   Expected Response Formatting: [NO] Tool returned clean price data...
   ```

2. **Test 2: Technical Analysis**
   ```
   Prompt: "Get TA indicators for NVDA"
   Expected Tools Used: get_ta_indicators(ticker='NVDA')
   Expected Response Formatting: [NO] Tool returned pre-formatted markdown table...
   ```

3. **Test 3: Options Chain**
   ```
   Prompt: "Show me SPY options chain expiring 2025-11-01"
   Expected Tools Used: get_stock_quote, get_options_chain_both
   Expected Response Formatting: [NO] Tool returned pre-formatted markdown table, preserved as-is per RULE #9
   ```

4. **Test 4: Multi-Ticker Quote**
   ```
   Prompt: "Get quotes for SPY, QQQ, IWM"
   Expected Tools Used: get_stock_quote(ticker='SPY,QQQ,IWM')
   Expected Response Formatting: [YES/NO] depending on agent's formatting decision
   ```

5. **Test 5: Follow-up Question (No Tool Calls)**
   ```
   Prompt 1: "Get quote for AAPL"
   Prompt 2: "What's the change percentage?"
   Expected Tools Used (Prompt 2): None (uses existing data)
   Expected Response Formatting: [N/A] No tool calls made, response based on existing data from chat history
   ```

**Verification Checklist (for EACH test):**
- ‚úÖ Response includes "Tools Used:" section
- ‚úÖ Response includes "Response Formatting:" section AFTER Tools Used
- ‚úÖ Response Formatting has [YES/NO/N/A] tag
- ‚úÖ Response Formatting includes explanation (not just tag)
- ‚úÖ For [YES]: Describes WHAT and WHY
- ‚úÖ For [NO]: Explains why preserved
- ‚úÖ For [N/A]: Confirms no tool calls
- ‚úÖ Response data is correct (no regressions)

**Manual Test Pass Criteria:**
- ALL 5 tests must include Response Formatting section
- ALL 5 tests must have [YES/NO/N/A] with explanation
- NO regressions in data accuracy or tool selection

**If Manual Tests Fail:**
- Review agent instruction syntax and placement
- Check for instruction ambiguity
- Verify examples are clear
- Re-test after fixes before proceeding to Phase 6

---

## Phase 6: Full Regression Testing

### Task 6.1: Run Full Test Suite

**Command:**
```bash
chmod +x test_cli_regression.sh && ./test_cli_regression.sh
```

**Expected Duration:** ~10 minutes (37 tests)

**Success Criteria:**
- ‚úÖ 37/37 tests complete (all responses received)
- ‚úÖ No crashes or runtime errors
- ‚úÖ Test report generated in test-reports/

**Monitoring:**
- Watch for errors during execution
- Note any tests that hang or timeout
- Record response times (should be similar to baseline)

---

### Task 6.2: Phase 2 Manual Verification (ALL 37 TESTS)

**üî¥ CRITICAL: This is the MANDATORY manual verification phase**

**Tool:** Use `Read` tool to read test log file

**Process:** For EACH of the 37 tests, apply 5-point verification:

#### Verification Criteria (Apply to EACH Test)

1. ‚úÖ **Response addresses the query?**
   - Does agent response answer the test prompt?
   - Is response relevant to ticker(s) mentioned?
   - Is response complete (not truncated)?

2. ‚úÖ **RIGHT tools called (no duplicates)?**
   - Are tools appropriate for the query?
   - No redundant API calls?
   - Follows RULE #6 (chat history check)?

3. ‚úÖ **Data correct?**
   - Correct ticker symbols?
   - Proper formatting (tables not broken)?
   - No hallucinated data?

4. ‚úÖ **No errors?**
   - No error messages in response?
   - No "data unavailable" messages?
   - No RuntimeWarnings?

5. ‚úÖ **Response Formatting section present and valid?** (NEW)
   - Section exists after "Tools Used"?
   - Has [YES/NO/N/A] tag?
   - Includes explanation (not just tag)?
   - For [YES]: Describes WHAT and WHY?
   - For [NO]: Explains why preserved?
   - For [N/A]: Confirms no tool calls?

#### Documentation Format

Create table documenting ALL 37 tests:

| Test # | Test Name | Tools | Resp Format | Status | Issue (if failed) |
|--------|-----------|-------|-------------|--------|-------------------|
| 1 | Market_Status | get_market_status_and_date_time | [NO] Tool returned ... | ‚úÖ PASS | - |
| 2 | SPY_Price | get_stock_quote | [NO] Tool returned ... | ‚úÖ PASS | - |
| ... | ... | ... | ... | ... | ... |

#### Checkpoint Questions (Answer ALL with Evidence)

1. ‚úÖ Did you READ all 37 test responses manually using the Read tool? **YES/NO**
2. ‚úÖ Did you apply all 5 verification criteria to EACH test? **YES/NO**
3. ‚úÖ How many tests PASSED all 5 criteria? **X/37 PASSED**
4. ‚úÖ How many tests FAILED Response Formatting criteria? **X/37 FAILED**
5. ‚úÖ How many tests have [YES] Response Formatting? **X/37**
6. ‚úÖ How many tests have [NO] Response Formatting? **X/37**
7. ‚úÖ How many tests have [N/A] Response Formatting? **X/37**
8. ‚úÖ Did you document ALL tests in results table? **YES/NO + TABLE**

**üî¥ CANNOT PROCEED to Phase 7 WITHOUT:**
- Reading all 37 test responses manually (using Read tool)
- Applying all 5 verification criteria to each test
- Documenting ALL 37 tests in results table
- Providing Response Formatting distribution ([YES] vs [NO] vs [N/A])
- Answering all 8 checkpoint questions with evidence

---

### Task 6.3: Analyze Response Formatting Patterns

**Goal:** Understand how agent is using new Response Formatting feature

**Analysis Questions:**

1. **Distribution Analysis:**
   - How many [YES] (reformatted)? Expected: 0-5 (minimal)
   - How many [NO] (preserved)? Expected: 30-35 (majority)
   - How many [N/A] (no tools)? Expected: 0-2 (rare)

2. **Quality Analysis:**
   - Are [YES] explanations specific about WHAT changed?
   - Are [NO] explanations clear about WHY preserved?
   - Do agents reference RULE #9 when preserving tables?
   - Are explanations consistent across similar test types?

3. **Pattern Analysis:**
   - Which test types get [YES] more often?
   - Are options chain tests consistently [NO]? (should be!)
   - Are TA indicator tests consistently [NO]? (should be!)
   - Any unexpected [YES] formatting that shouldn't happen?

**Expected Patterns:**
- Most tests: [NO] with "Tool returned pre-formatted markdown table"
- Options chains: [NO] with reference to RULE #9
- Simple quotes: [NO] with "Tool returned clean price data"
- Complex multi-tool responses: Possibly [YES] if agent synthesizes

**Red Flags:**
- ‚ùå Options chain marked [YES] (agent shouldn't reformat these!)
- ‚ùå TA indicators marked [YES] (tables should be preserved!)
- ‚ùå Missing explanations (just [YES] or [NO] alone)
- ‚ùå Vague explanations ("formatted for clarity" - too vague!)

---

## Phase 7: Final Git Commit Phase

### Task 7.1: Pre-Commit Verification

**Action:** Verify ALL work is complete before staging

**Checklist:**
- ‚úÖ Code changes complete (agent_service.py modified)
- ‚úÖ Manual testing complete (5 prompts, all passed)
- ‚úÖ Full regression testing complete (37/37 tests passed)
- ‚úÖ Phase 2 manual verification complete (all 37 tests reviewed)
- ‚úÖ Response Formatting analysis complete
- ‚úÖ Test report generated and saved
- ‚úÖ All checkpoint questions answered

**Files to Verify:**

```bash
# Review all changed files
git status

# Expected changes:
# M src/backend/services/agent_service.py
# ?? test-reports/test_cli_regression_loop1_2025-10-29_XX-XX.log
# M research_task_plan.md
# M TODO_task_plan.md
```

**DO NOT STAGE YET** - Wait until all documentation is updated

---

### Task 7.2: Update Documentation

#### 7.2.1 Update CLAUDE.md Last Task Summary

**File:** `CLAUDE.md`
**Section:** `<!-- LAST_COMPLETED_TASK_START -->`

**Required Content (20 lines maximum):**
```markdown
[RESPONSE_FORMATTING_TRANSPARENCY] Add Response Formatting transparency to agent responses

**Summary:** Successfully added "Response Formatting" transparency metadata to AI Agent responses, documenting whether and why tool outputs are reformatted. Feature helps debug formatting issues by making agent's formatting decisions explicit.

**Implementation:**
- ‚úÖ Added Response Formatting instruction block to agent_service.py (35 lines, +200 tokens)
- ‚úÖ Positioned after existing "Tools Used" section for consistent metadata pattern
- ‚úÖ Format: [YES/NO/N/A] with required explanation of formatting decisions
- ‚úÖ Manual testing: 5 prompts validated (all show Response Formatting section)
- ‚úÖ Full regression testing: 37/37 tests passed with 100% Response Formatting presence
- ‚úÖ Distribution: X [YES], Y [NO], Z [N/A] across 37 tests
- ‚úÖ No regressions in tool selection, data accuracy, or response quality

**Files Modified:**
- ‚úÖ src/backend/services/agent_service.py (agent instructions updated)
- ‚úÖ test-reports/test_cli_regression_loop1_2025-10-29_XX-XX.log (test evidence)

**Key Features:**
- Explicit [YES] tag when agent reformats tool output (with WHAT and WHY)
- Explicit [NO] tag when agent preserves tool output (with reasoning)
- [N/A] tag when no tool calls made (using existing data)
- Always includes explanation - never just tag alone
- References existing rules (RULE #9) when applicable

**Risk Assessment:** VERY LOW ‚úÖ (instruction-only change, no code logic modified)
**Testing Evidence:** 37/37 tests passed, test report: test-reports/test_cli_regression_loop1_2025-10-29_XX-XX.log
```

**Verification:**
- ‚úÖ Summary is 20 lines or less
- ‚úÖ Includes test distribution (X [YES], Y [NO], Z [N/A])
- ‚úÖ Includes test report file path
- ‚úÖ Mentions no regressions

---

#### 7.2.2 Update research_task_plan.md Status

**File:** `research_task_plan.md`
**Action:** Add completion status at top

**Add to top of file:**
```markdown
# Research Task Plan: Response Formatting Transparency Feature

**Status:** ‚úÖ COMPLETED
**Implementation Date:** 2025-10-29
**Test Results:** 37/37 tests passed
**Distribution:** X [YES], Y [NO], Z [N/A]
**Test Report:** test-reports/test_cli_regression_loop1_2025-10-29_XX-XX.log

---

[Rest of research plan content...]
```

---

#### 7.2.3 Update TODO_task_plan.md Status

**File:** `TODO_task_plan.md` (this file)
**Action:** Mark all phases as complete

**Update phase statuses:**
- Phase 1: Research ‚úÖ COMPLETED
- Phase 2: Planning ‚úÖ COMPLETED
- Phase 3: User Approval ‚úÖ APPROVED
- Phase 4: Implementation ‚úÖ COMPLETED
- Phase 5: Manual Testing ‚úÖ COMPLETED (5/5 tests passed)
- Phase 6: Full Regression Testing ‚úÖ COMPLETED (37/37 tests passed)
- Phase 7: Final Commit ‚úÖ IN PROGRESS

---

### Task 7.3: Stage ALL Files at Once

**üî¥ CRITICAL: Stage ONLY when ALL work is complete**

**Command:**
```bash
git add -A
```

**This stages:**
- ‚úÖ src/backend/services/agent_service.py (code changes)
- ‚úÖ test-reports/test_cli_regression_loop1_2025-10-29_XX-XX.log (test evidence)
- ‚úÖ CLAUDE.md (documentation updated)
- ‚úÖ research_task_plan.md (research documentation)
- ‚úÖ TODO_task_plan.md (implementation plan)

**Verify staging immediately:**
```bash
git status
```

**Expected output:**
```
Changes to be committed:
  modified:   CLAUDE.md
  modified:   src/backend/services/agent_service.py
  modified:   research_task_plan.md
  modified:   TODO_task_plan.md
  new file:   test-reports/test_cli_regression_loop1_2025-10-29_XX-XX.log
```

**‚ùå IF ANY FILES ARE MISSING:** `git add [missing-file]`

---

### Task 7.4: Create Atomic Commit

**üî¥ CRITICAL: Commit IMMEDIATELY after staging (within 60 seconds)**

**Command:**
```bash
git commit -m "$(cat <<'EOF'
[RESPONSE_FORMATTING_TRANSPARENCY] Add Response Formatting transparency to agent responses

- Add Response Formatting instruction block to agent_service.py
- Position after "Tools Used" section for consistent metadata pattern
- Format: [YES/NO/N/A] with required explanation of formatting decisions
- [YES]: Agent reformatted tool output (describes WHAT and WHY)
- [NO]: Agent preserved tool output (explains reasoning)
- [N/A]: No tool calls made (using existing data)
- Manual testing: 5 prompts validated
- Full regression testing: 37/37 tests passed with 100% Response Formatting presence
- Distribution: X [YES], Y [NO], Z [N/A] across 37 tests
- No regressions in tool selection, data accuracy, or response quality
- Test report: test-reports/test_cli_regression_loop1_2025-10-29_XX-XX.log

Files Modified:
- src/backend/services/agent_service.py (agent instructions)
- CLAUDE.md (Last Task Summary updated)
- research_task_plan.md (research documentation)
- TODO_task_plan.md (implementation plan)
- test-reports/ (test evidence)

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
EOF
)"
```

**Success Indicators:**
- ‚úÖ Commit created with detailed message
- ‚úÖ All files included in single atomic commit
- ‚úÖ No uncommitted changes remain

---

### Task 7.5: Push to Remote

**Command:**
```bash
git push
```

**Success Criteria:**
- ‚úÖ Push succeeds without conflicts
- ‚úÖ Remote branch updated
- ‚úÖ No error messages

**If push fails:**
- Review error message
- If conflict: Pull and resolve
- If authentication: Check credentials
- Retry push after resolution

---

## Phase 8: Final Verification

### Task 8.1: Confirm Task Completion

**Final Checklist:**

- ‚úÖ Research completed (Phase 1)
- ‚úÖ Planning completed (Phase 2)
- ‚úÖ User approved plans (Phase 3)
- ‚úÖ Agent instructions updated (Phase 4)
- ‚úÖ Manual testing passed 5/5 (Phase 5)
- ‚úÖ Full regression testing passed 37/37 (Phase 6)
- ‚úÖ Phase 2 manual verification completed (Phase 6)
- ‚úÖ Response Formatting distribution analyzed (Phase 6)
- ‚úÖ Documentation updated (Phase 7)
- ‚úÖ Atomic commit created (Phase 7)
- ‚úÖ Pushed to remote (Phase 7)

**Success Message:**
```
‚úÖ Response Formatting Transparency Feature - COMPLETE

Implementation Summary:
- Agent instructions updated with Response Formatting transparency
- Manual testing: 5/5 tests passed
- Full regression testing: 37/37 tests passed
- Response Formatting presence: 100% (all 37 tests)
- Distribution: X [YES], Y [NO], Z [N/A]
- No regressions detected
- Atomic commit pushed to remote

Feature Benefits:
- Explicit documentation of agent formatting decisions
- Easy debugging of formatting issues
- Transparency about when/why tool outputs are reformatted
- Consistent with existing "Tools Used" transparency pattern

Ready for production use ‚úÖ
```

---

## Task Dependencies

```
Phase 1 (Research)
    ‚Üì
Phase 2 (Planning)
    ‚Üì
Phase 3 (User Approval) ‚Üê GATE
    ‚Üì
Phase 4 (Implementation)
    ‚Üì
Phase 5 (Manual Testing) ‚Üê GATE
    ‚Üì
Phase 6 (Full Regression Testing) ‚Üê GATE
    ‚Üì
Phase 7 (Documentation + Commit)
    ‚Üì
Phase 8 (Final Verification)
```

**GATES:**
- Phase 3: Cannot proceed without user approval
- Phase 5: Cannot proceed without 5/5 manual tests passing
- Phase 6: Cannot proceed without 37/37 tests passing + manual verification

---

## Risk Mitigation

### Low Risk Items ‚úÖ
- Instruction-only change (no code logic)
- Leverages existing transparency pattern
- Simple format ([YES/NO/N/A] + explanation)
- Reversible (can revert commit if issues)

### Potential Issues & Solutions

**Issue 1: Agent doesn't include Response Formatting section**
- **Cause:** Instruction ambiguity or placement issue
- **Solution:** Review instruction text, add more examples, emphasize "ALWAYS"

**Issue 2: Agent outputs just [YES/NO] without explanation**
- **Cause:** Instruction not clear about explanation requirement
- **Solution:** Strengthen "MUST explain" language in instructions

**Issue 3: Response Formatting breaks tool output**
- **Cause:** Agent misinterprets when to format
- **Solution:** Clarify [NO] should be default for pre-formatted outputs

**Issue 4: Tests fail after implementation**
- **Cause:** Instruction conflicts with existing rules
- **Solution:** Review rule consistency, adjust if needed

---

## Rollback Plan

**If Implementation Fails:**

1. **Immediate Rollback:**
   ```bash
   git revert HEAD
   git push
   ```

2. **Analyze Failure:**
   - Review test failures
   - Check agent instruction conflicts
   - Identify root cause

3. **Fix and Retry:**
   - Adjust instruction text
   - Re-test manually
   - Re-run full regression suite
   - Create new commit with fixes

---

## Estimated Timeline

| Phase | Duration | Cumulative |
|-------|----------|------------|
| Phase 1: Research | 20 min | 20 min |
| Phase 2: Planning | 10 min | 30 min |
| Phase 3: User Approval | 5 min | 35 min |
| Phase 4: Implementation | 15 min | 50 min |
| Phase 5: Manual Testing | 15 min | 65 min |
| Phase 6: Full Regression | 60 min | 125 min |
| Phase 7: Documentation + Commit | 15 min | 140 min |
| Phase 8: Verification | 5 min | 145 min |

**Total Estimated Duration:** 2.5 hours

---

## Success Criteria Summary

**MUST HAVE (Required for Success):**
- ‚úÖ Response Formatting section appears in 100% of responses
- ‚úÖ All responses have [YES/NO/N/A] with explanation
- ‚úÖ No regressions in tool selection or data accuracy
- ‚úÖ 37/37 tests pass manual verification (all 5 criteria)
- ‚úÖ Test evidence documented in test report

**NICE TO HAVE (Quality Indicators):**
- ‚úÖ Majority of responses are [NO] (preserving pre-formatted outputs)
- ‚úÖ [YES] responses have specific WHAT and WHY details
- ‚úÖ Agent references RULE #9 when preserving tables
- ‚úÖ Explanations are consistent across similar test types

**FAILURE CONDITIONS (Trigger Rollback):**
- ‚ùå More than 2 tests missing Response Formatting section
- ‚ùå More than 5 tests with regressions (tool selection, data errors)
- ‚ùå Agent consistently reformats pre-formatted tables ([YES] when should be [NO])
- ‚ùå Explanations are vague or missing (just [YES/NO] tags)

---

## End of TODO Task Plan

**Current Status:** Phase 2 Complete, Awaiting User Approval (Phase 3)
**Next Action:** Present plans to user, wait for approval
**Implementation Ready:** YES ‚úÖ
