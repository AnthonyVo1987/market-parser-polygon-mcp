# Research Task Plan: Response Formatting Transparency Feature

**Status: âœ… COMPLETED (2025-10-29)** - All research phases finished, implementation validated with 37/37 tests passing

## Research Objective

Research how to add "Response Formatting" transparency metadata to AI Agent responses, similar to the existing "Tools Used" transparency section. This feature will help debug AI Agent response formatting issues by making it explicit when and why the agent reformats tool outputs.

---

## Phase 1: Understanding OpenAI Agents SDK Patterns

### 1.1 Agent Response Structure Research

**Tools Used:**
- `mcp__docs-openai-agents__search_openai_agents_docs` (3 queries)
- `mcp__docs-openai-agents__search_openai_agents_code` (2 queries)
- `mcp__docs-openai-agents__fetch_generic_url_content` (2 examples)

**Key Findings:**

1. **Agent Instructions Are Simple Strings**
   - The `instructions` parameter in `Agent()` accepts plain text strings
   - No special syntax required - just natural language guidance
   - Example from OpenAI docs:
     ```python
     agent = Agent(
         name="Assistant",
         instructions="You are a helpful assistant. Keep responses conversational."
     )
     ```

2. **Structured Output Patterns**
   - Agents can be instructed to produce specific output formats
   - Example from "LLM as a judge" pattern:
     ```python
     evaluator = Agent(
         name="evaluator",
         instructions=(
             "You evaluate a story outline and decide if it's good enough. "
             "If it's not good enough, you provide feedback..."
         ),
         output_type=EvaluationFeedback
     )
     ```
   - This shows agents can follow instructions about HOW to structure responses

3. **Agent Self-Reflection Capability**
   - Agents can be instructed to evaluate their own outputs
   - Pattern: Provide explicit instructions about what metadata to include
   - No special SDK features needed - just clear instructions

4. **Response Metadata Patterns**
   - OpenAI Agents SDK supports `RunResult` with `final_output` and `new_items`
   - Custom metadata can be added via agent instructions (not SDK features)
   - Tracing is separate (external debugging), not response metadata

**Conclusion:** Agent instructions are the RIGHT place to add Response Formatting transparency. No SDK changes needed - just instruction updates.

---

## Phase 2: Current System Architecture Analysis

### 2.1 Agent Service Structure

**File Analyzed:** `src/backend/services/agent_service.py`

**Key Symbols:**
1. `get_enhanced_agent_instructions()` - Returns agent instruction string
2. `get_optimized_model_settings()` - Model configuration
3. `create_agent()` - Agent factory function

**Current Instruction Structure:**

```python
def get_enhanced_agent_instructions():
    datetime_context = get_current_datetime_context()
    return f"""You are a financial analyst with real-time market data access.

{datetime_context}

ðŸ“‹ COMMON FORMATS (Reference for all tools and rules):
- Date Format: YYYY-MM-DD
- Multi-Ticker Format: Comma-separated, no spaces
- Table Format: Markdown tables with pipe separators

ðŸ”´ðŸ”´ðŸ”´ CRITICAL TOOL SELECTION RULES - READ CAREFULLY ðŸ”´ðŸ”´ðŸ”´

RULE #1: STOCK QUOTES = USE get_stock_quote()
[... 9 total rules ...]

ðŸ”§ TOOL CALL TRANSPARENCY REQUIREMENT:
At the END of EVERY response, you MUST include a "Tools Used" section...

Example format:
---
**Tools Used:**
- `tool_name(parameters)` - Reasoning for selection
"""
```

**Key Observations:**

1. **Already Has Transparency Pattern**: "Tools Used" section already exists!
2. **Instruction Size**: ~2000+ tokens (detailed, comprehensive)
3. **Structured Format**: Clear sections with emojis, rules, examples
4. **End-of-Response Metadata**: Already established pattern at response end
5. **Token Efficiency**: Already optimized with "COMMON FORMATS" centralization

**Perfect Insertion Point:** Right after "TOOL CALL TRANSPARENCY REQUIREMENT"

---

## Phase 3: Response Formatting Metadata Design

### 3.1 Format Requirements (from user)

User requested:
1. Add "Response Formatting Details" after "Tools Used" section
2. If response was formatted: State WHAT was changed and WHY
3. If response was NOT formatted: State WHY no formatting was needed
4. Use clear [YES/NO] indicators

### 3.2 Proposed Format Design

**Format Pattern:**

```
**Response Formatting:**
[YES/NO/N/A] Explanation of formatting decision
```

**Three Scenarios:**

1. **[YES] - Agent Reformatted Output**
   ```
   **Response Formatting:**
   [YES] Converted raw options chain data into structured markdown table with Bid/Ask columns for better readability and comparison
   ```

2. **[NO] - Agent Preserved Output**
   ```
   **Response Formatting:**
   [NO] Tool returned pre-formatted markdown table that already meets display requirements, preserved as-is per RULE #9
   ```

3. **[N/A] - No Tool Calls**
   ```
   **Response Formatting:**
   [N/A] No tool calls made, response based on existing data from chat history
   ```

**Design Rationale:**

- **Binary Decision First**: [YES/NO/N/A] for easy parsing and debugging
- **Always Explain**: Never just [YES] alone - always provide reasoning
- **Reference Rules**: Connect to existing rules (like RULE #9) for context
- **Tool Output Context**: Describe original format and transformation
- **Consistent Pattern**: Matches existing "Tools Used" format
- **Specific Details**: Avoid vague statements like "formatted for clarity"

---

## Phase 4: Implementation Strategy

### 4.1 Instruction Block to Add

**Location:** In `get_enhanced_agent_instructions()`, after "TOOL CALL TRANSPARENCY REQUIREMENT"

**Proposed Text:**

```python
ðŸ“‹ RESPONSE FORMATTING TRANSPARENCY REQUIREMENT:

At the END of EVERY response, IMMEDIATELY AFTER the "Tools Used" section, you MUST include a "Response Formatting" section that documents your formatting decisions:

**Response Formatting:**
[YES/NO/N/A] Explanation of formatting decision

FORMAT OPTIONS:

1. [YES] - You reformatted or restructured tool output
   - MUST describe WHAT changes you made (e.g., "Converted raw data into markdown table")
   - MUST explain WHY you made those changes (e.g., "for better readability")
   - Example: "[YES] Converted raw options chain data into structured markdown table with Bid/Ask columns for better readability"

2. [NO] - You preserved tool output exactly as returned
   - MUST explain WHY no formatting was needed (e.g., "Tool output already in optimal format")
   - Example: "[NO] Tool returned pre-formatted markdown table, preserved as-is per RULE #9"

3. [N/A] - No tool calls were made (using existing data)
   - Use when no new tool data to format
   - Example: "[N/A] No tool calls made, response based on existing data from chat history"

CRITICAL RULES:
- ðŸ”´ ALWAYS include this section in EVERY response (no exceptions)
- ðŸ”´ Place AFTER "Tools Used" section (not before)
- ðŸ”´ Be specific about WHAT formatting changes were made
- ðŸ”´ NEVER say just "[YES]" or "[NO]" without explanation

Example complete response ending:
---
**Tools Used:**
- `get_stock_quote(ticker='SPY')` - Single ticker request per RULE #1

**Response Formatting:**
[NO] Tool returned clean price data that was already structured, no reformatting applied
```

### 4.2 Token Impact Analysis

**Current Instruction Size:** ~2000+ tokens
**New Section Size:** ~200 tokens (estimated)
**Total After Addition:** ~2200 tokens
**Impact:** +10% increase (acceptable for improved debugging)

### 4.3 Testing Strategy

**Test File:** `test_cli_regression.sh` (37 tests)

**Test Modifications Needed:**

1. **Phase 2 Verification Enhancement:**
   - Current: Check for errors, tool calls, data correctness
   - Add: Verify "Response Formatting:" section exists in EVERY response
   - Add: Verify format is [YES/NO/N/A] with explanation (not just tag)

2. **Manual Verification Checklist:**
   - Does response include "Response Formatting:" section? âœ“
   - Does it have [YES/NO/N/A] tag? âœ“
   - Does it include explanation (not just tag alone)? âœ“
   - For [YES]: Does it describe WHAT and WHY? âœ“
   - For [NO]: Does it explain why preserved? âœ“
   - Is it placed AFTER "Tools Used" section? âœ“

3. **Expected Test Results:**
   - All 37 tests should have Response Formatting section
   - Most will be [NO] (preserving pre-formatted tool outputs)
   - Some will be [YES] (when agent simplifies or restructures)
   - None should be [N/A] (all tests make tool calls)

---

## Phase 5: Integration Points

### 5.1 Files to Modify

1. **src/backend/services/agent_service.py**
   - Function: `get_enhanced_agent_instructions()`
   - Change: Add new instruction block after TOOL CALL TRANSPARENCY
   - Location: After line ~259 (end of current instructions)

2. **test_cli_regression.sh**
   - Add Phase 2 verification for Response Formatting section
   - Update expected output patterns

3. **CLAUDE.md**
   - Update "Last Completed Task Summary" with feature details
   - Document new Response Formatting transparency requirement

### 5.2 No Changes Needed

- âœ… Tool functions (tradier_tools.py, polygon_tools.py) - No changes
- âœ… CLI/Gradio interfaces - No changes (transparent to user)
- âœ… Test prompts - No changes (same test scenarios)
- âœ… Agent factory function - No changes (instructions only)

---

## Research Conclusions

### Key Findings Summary

1. **Solution is Simple**: Just add natural language instructions - no SDK changes needed
2. **Existing Pattern**: "Tools Used" transparency already establishes response metadata pattern
3. **Perfect Fit**: New "Response Formatting" section complements existing transparency
4. **Low Risk**: Instruction-only change, no code logic modifications
5. **High Value**: Provides exact debugging info needed to identify formatting issues

### Implementation Confidence: HIGH âœ…

**Reasons:**
- Leverages existing, proven pattern (Tools Used transparency)
- Simple instruction addition (no complex code changes)
- Clear test validation strategy
- Minimal token impact (+200 tokens)
- Direct solution to options chain formatting debugging issue

### Next Steps

1. Create TODO_task_plan.md with detailed implementation steps
2. Present plans to user for approval
3. Upon approval, proceed with implementation
4. Run manual testing first (3-5 prompts)
5. Run full regression test suite (37 tests)
6. Manual verification of all 37 test responses
7. Create atomic commit with all changes

---

## Research Artifacts

**Documentation Consulted:**
- OpenAI Agents Python SDK documentation (via mcp__docs-openai-agents__)
- OpenAI Python SDK documentation (via mcp__docs-openai-python__)
- Example code: output_guardrails.py, llm_as_a_judge.py

**Code Analyzed:**
- src/backend/services/agent_service.py (get_enhanced_agent_instructions)
- Current instruction structure and patterns
- Existing "Tools Used" transparency implementation

**Research Duration:** ~20 minutes
**Research Quality:** Comprehensive âœ…
**Ready for Implementation:** YES âœ…
