# Playwright Testing Master Plan

## Playwright Tools Testing Guide

**Version**: 1.0  
**Last Updated**: 2025-01-10  
**Status**: Production Ready  

---

## üéØ Executive Summary

This document serves as the **single source of truth** for Playwright testing of the Market Parser application using Playwright Tools methodology, providing complete guidance for executing comprehensive test validation.

### Key Capabilities

- **Playwright Tools Support**: Playwright Tools approach executes comprehensive test validation
- **100% Success Rate**: Playwright Tools methodology achieved complete functional validation
- **Performance Optimized**: Playwright Tools (50s average with optimization opportunities)
- **AI Agent Ready**: Complete Playwright Tools implementation for automated testing workflows
- **Single Browser Session**: Real-world simulation maintaining state continuity

### Critical Success Factors

1. **Unified Test Specifications**: Playwright Tools methodology validates comprehensive functionality
2. **Single Browser Session Rule**: All tests execute in one continuous browser instance
3. **Performance Baseline Monitoring**: Established timing expectations for regression detection
4. **Comprehensive Error Handling**: Robust failure recovery and meaningful reporting

---

## ü§ñ Playwright Tools Testing Methodology

### Prerequisites

```bash
# Verify Playwright Tools availability
# Note: These are provided by the Playwright Tools environment

# Required Playwright Tools:
# - mcp__playwright__browser_navigate
# - mcp__playwright__browser_snapshot
# - mcp__playwright__browser_click
# - mcp__playwright__browser_type
# - mcp__playwright__browser_close
```

### Implementation Checklist

#### Phase 1: Playwright Tools Environment Validation

- [ ] **Playwright Tools**: All playwright tools accessible
- [ ] **Backend Server**: FastAPI running on localhost:8000
- [ ] **Frontend Server**: React app running on localhost:3000
- [ ] **Browser Integration**: Playwright browser tools functional

#### Phase 2: Test Execution Protocol

```
1. run start up scripts // TODO
2. Initialize browser session via mcp__playwright__browser_navigate
3. Validate each test step with mcp__playwright__browser_snapshot
4. Handle interactions via mcp__playwright__browser_click/type
5. Close browser session via mcp__playwright__browser_close
```

#### Phase 2.1: AI Agent Report Generation Requirements

**CRITICAL**: AI agents MUST follow these exact requirements to prevent formatting errors:

- **VERBATIM CAPTURE**:
  - **Test Input**: Must capture the EXACT user input message or button name
  - **Test Output**: Must capture the COMPLETE AI response text verbatim
  - **NO SUMMARIES**: Do not summarize or truncate responses

- **TEMPLATE COMPLIANCE**:
  - **EXACT FORMAT**: Follow template structure precisely without modifications
  - **REQUIRED SECTIONS**: Include all mandatory sections in exact order
  - **NO DEVIATIONS**: Do not add, remove, or modify template sections

- **NAMING CONVENTIONS**:
  - **REPORT NAMING**: Use `Playwright_Tools_Test_Report__YY-MM-DD_hh-mm.md` (double underscore)
  - **TEST NUMBERING**: Use `### Test [X]: [Test Name]` format
  - **CONSISTENCY**: Maintain consistent formatting throughout report

- **TIMESTAMP DETECTION**:
  - **MANDATORY TOOL USAGE**: Use `run_terminal_cmd` with exact commands specified
  - **REAL-WORLD TIMESTAMPS**: Never use training data cutoff dates
  - **PACIFIC TIME**: Always use `TZ='America/Los_Angeles'` for timezone

#### Phase 3: State Management

- [ ] **Session Continuity**: Single browser instance maintained
- [ ] **State Preservation**: Application state carried between tests
- [ ] **Error Recovery**: Graceful handling of individual test failures
- [ ] **Performance Monitoring**: Execution timing within expected ranges

### Expected Performance Baseline

- **Total Execution Time**: 50s ¬± 7s
- **Success Rate**: 100% functional validation
- **Browser Sessions**: 1 continuous session
- **Playwright Tool Calls**: ~150-200 individual operations

---

## üìã Test Specifications

### Core Application Tests

#### System Startup and Health Verification

- **Purpose**: Validate backend and frontend system availability
- **Success Criteria**:
  - Backend health endpoint returns 200 status
  - Frontend loads without console errors
  - API connectivity established
- **Expected Duration**: 2-3 seconds
- **Validation**: Health endpoints respond correctly

#### Frontend Loading and Basic Functionality

- **Purpose**: Verify React application loads and renders correctly
- **Success Criteria**:
  - Main chat interface visible
  - Input elements accessible
  - No JavaScript runtime errors
- **Expected Duration**: 1-2 seconds
- **Validation**: DOM elements present and functional

#### API Endpoint Connectivity Validation

- **Purpose**: Confirm backend-frontend communication
- **Success Criteria**:
  - API endpoints reachable
  - CORS configuration correct
  - Basic request-response cycle functional
- **Expected Duration**: 1-2 seconds
- **Validation**: Successful API communication

---

## üìã Test Report Format Requirements

### Report Generation Standards

**Mandatory**: All Playwright Tools test executions MUST generate standardized test reports following the specifications below.

#### Report Naming Convention

```
Playwright_Tools_Test_Report__YY-MM-DD_hh-mm.md
```

**‚ö†Ô∏è NAMING CONVENTION REQUIREMENTS:**

- **Format**: Use **DOUBLE UNDERSCORE** between "Report" and date
- **Example**: `Playwright_Tools_Test_Report__25-09-19_18-47.md`
- **MUST** follow this exact format for consistency

**Timestamp Requirements:**

- **Format**: Pacific Time (PT/PST) in YY-MM-DD_hh-mm format
- **Detection**: **MANDATORY** - Use `run_terminal_cmd` tool to execute: `TZ='America/Los_Angeles' date '+%y-%m-%d_%H-%M'`
- **Real-World**: **CRITICAL** - NEVER use training data cutoff dates. ALWAYS detect actual current timestamp using system commands
- **Validation**: The timestamp MUST be from the actual execution date, not from AI training data

**‚ö†Ô∏è CRITICAL WARNING FOR AI AGENTS:**

- **DO NOT** use your training data cutoff date as the "current" date
- **DO NOT** assume or guess the current date
- **MUST** use the `run_terminal_cmd` tool to get real-world Pacific timestamp
- **MUST** execute the command: `TZ='America/Los_Angeles' date '+%y-%m-%d_%H-%M'`
- **MUST** use the exact output from this command in the filename

**Example Command Execution:**

```bash
TZ='America/Los_Angeles' date '+%y-%m-%d_%H-%M'
# Output: 25-09-19_18-01
# Use this EXACT output in filename: Playwright_Tools_Test_Report_25-09-19_18-01.md
```

- `test-reports/Playwright_Tools_Test_Report_25-09-19_18-01.md` - Example format (use actual detected timestamp)

#### Save Location

**Mandatory Directory**: `test-reports/`

**Directory Structure:**

```

 test_reports/
    ‚îú‚îÄ‚îÄ Playwright_Tools_Test_Report_25-09-19_18-01.md
    ‚îî‚îÄ‚îÄ [additional reports...]
```

### Report Template Structure

#### Header Section (Required)

```markdown
# Playwright Testing Report - Playwright Tools Methodology

**Execution Date**: [YYYY-MM-DD] - **MUST** use `run_terminal_cmd` with `TZ='America/Los_Angeles' date '+%Y-%m-%d'`
**Execution Time**: [HH:MM PT/PST] - **MUST** use `run_terminal_cmd` with `TZ='America/Los_Angeles' date '+%H:%M %Z'`
**Methodology**: Playwright Tools
**Test Suite**: [Test Suite Name] (X Tests)
**Total Tests**: [X]
**Success Rate**: [X/X] ([XX%])
**Total Execution Time**: [XX.X]s
**Browser Sessions**: 1 (Continuous)

**‚ö†Ô∏è CRITICAL TIMESTAMP REQUIREMENTS:**
- **DO NOT** use training data cutoff dates
- **MUST** execute: `TZ='America/Los_Angeles' date '+%Y-%m-%d'` for Execution Date
- **MUST** execute: `TZ='America/Los_Angeles' date '+%H:%M %Z'` for Execution Time
- **MUST** use actual system-detected timestamps, not assumed dates

**‚ö†Ô∏è CRITICAL AI AGENT REQUIREMENTS:**
- **VERBATIM INPUT/OUTPUT**: **MUST** capture exact user input and complete AI response text
- **TEMPLATE COMPLIANCE**: **MUST** follow exact template format without modifications
- **NAMING PRECISION**: **MUST** use double underscore in report naming: `Playwright_Tools_Test_Report__YY-MM-DD_hh-mm.md`
- **NO DEVIATIONS**: **MUST** follow template structure exactly as specified
```

**Key improvements:**

1. **Explicit commands** - Shows exactly what to run for each timestamp field
2. **Separate commands** - Date and time detection in separate commands for clarity
3. **Critical warning** - Prevents AI from using training data dates
4. **Methodology update** - Changed from "MCP" to "Playwright Tools" for consistency
5. **Bold formatting** - Makes requirements impossible to miss
6. **Specific tool requirement** - Forces use of `run_terminal_cmd`

This ensures AI agents will actually detect the real-world Pacific timestamps instead of using their training data cutoff dates.

---

## üéØ Executive Summary

[2-3 sentence summary of test execution results]

**Key Findings:**

- ‚úÖ/‚ùå [Primary success/failure indicator]
- ‚è±Ô∏è **Performance**: [Within/Outside] expected baseline ([XX.X]s vs [baseline]s)
- üîß **System Health**: [Status description]
- üìä **Coverage**: [X/16] tests completed successfully

---

```

#### Environment Configuration Section (Required)

```markdown
## üñ•Ô∏è Environment Configuration

**System Information:**
- **OS**: [Operating System]
- **Browser**: [Browser version]
- **Node.js**: [Version]
- **Playwright**: [Version]

**Service Configuration:**
- **Backend Server**: http://localhost:[PORT] (Status: [RUNNING/FAILED])
- **Frontend Server**: http://localhost:[PORT] (Status: [RUNNING/FAILED])
- **API Health**: [PASS/FAIL] - [Response details]

**Dynamic Port Configuration:**
- **Backend Port**: [8000/Custom] ([Auto-detected/Configured])
- **Frontend Port**: [3000/Custom] ([Auto-detected/Configured])
- **Port Conflicts**: [None/Resolved/Issues]

```

#### Granular Test Results (Required for Each B001-B016)

**Template for Each Test:**

**Examples of what should be captured:**

**For Chat Input Tests:**

- **Test Input**: "Market Status: PRIORITY FAST REQUEST NEEDING QUICK RESPONSE WITH MINIMAL TOOL CALLS ONLY & LOW Verbosity"
- **Test Output**: "KEY TAKEAWAYS\n\n‚Ä¢ Market Status: [Full response content]..."

**For Button Tests:**

- **Test Input**: "Stock Snapshot"
- **Test Output**: "KEY TAKEAWAYS\n\n‚Ä¢ Stock Analysis: [Full response content]..."

This ensures complete traceability and allows users to review exactly what was sent and what was received for each test.

```markdown
### Test [X]: [Test Name]
**Status**: ‚úÖ PASS / ‚ùå FAIL / ‚ö†Ô∏è PARTIAL
**Test Input**: [VERBATIM Chat Input Message OR Button Name]
**Test Output**: [VERBATIM AI Response - Complete Response Text]
**Duration**: [X.X]s
**Timeout**: 120s (Standard)
**Execution Time**: [Actual timing]

**Test Validation:**
- **[Specific validation point 1]**: ‚úÖ/‚ùå [Details]

**Performance Metrics:**
- **Expected Duration**: [X-Y]s
- **Actual Duration**: [X.X]s

**Error Details (If Applicable):**
```

[Error messages, stack traces, or failure descriptions]

```

**Screenshots/Evidence:**
- **Evidence Location**: [File paths or descriptions]

---
```

#### Error Analysis and Recovery (Required)

```markdown
## üîç Error Analysis and Recovery

**Error Summary:**
- **Total Errors**: [X]
- **Critical Errors**: [X] (Test failures)
- **Warning Errors**: [X] (Performance issues)
- **Recoverable Errors**: [X] (Handled gracefully)

**Error Categories:**

### System Errors
- **Backend Connection**: [Details]
- **Frontend Loading**: [Details]
- **API Communication**: [Details]
- **Browser Automation**: [Details]

### Test-Specific Errors
[List each failed test with specific error analysis]

### Recovery Actions Taken
- **Automatic Recovery**: [List successful recoveries]
- **Manual Intervention**: [List manual fixes required]
- **Unresolved Issues**: [List continuing problems]

### Recommendations
- **Immediate Actions**: [High priority fixes]
- **Short-term Improvements**: [Performance optimizations]
- **Long-term Enhancements**: [Architectural improvements]
```

#### Test Coverage and Quality Assurance (Required)

```markdown
## ‚úÖ Test Coverage and Quality Assurance

**Coverage Analysis:**

| Test Category | Tests | Passed | Failed | Coverage |
|---------------|-------|--------|--------|----------|


**Critical Path Validation:**
- ‚úÖ/‚ùå **System Startup**: Backend + Frontend initialization
- ‚úÖ/‚ùå **User Interaction**: Input ‚Üí Processing ‚Üí Response flow
- ‚úÖ/‚ùå **Data Integration**: API connectivity and data processing
- ‚úÖ/‚ùå **State Management**: Session and application state handling
```

#### Recommendations and Next Actions (Required)

```markdown
## üéØ Recommendations and Next Actions

### Immediate Actions (Next 24 Hours)

**Critical Issues:**
- [List any critical issues requiring immediate attention]

**Quick Wins:**
- [List simple improvements that can be implemented quickly]

### Short-term Improvements (Next Week)

**Performance Optimizations:**
- [List performance improvements with expected impact]

**Reliability Enhancements:**
- [List stability improvements for test execution]

### Long-term Enhancements (Next Month)

**Infrastructure Improvements:**
- [List architectural or infrastructure improvements]

**Test Suite Evolution:**
- [List additions or modifications to the test suite]

### Monitoring and Alerting

**Performance Monitoring:**
- **Setup**: [Recommendations for ongoing performance tracking]
- **Thresholds**: [Suggested alert thresholds based on current baseline]
- **Reporting**: [Frequency and format for regular performance reports]

**Quality Assurance:**
- **Automated Validation**: [Suggestions for automated QA integration]
- **Manual Review**: [Schedule for manual review and validation]
- **Continuous Improvement**: [Process for incorporating lessons learned]
```

#### Report Generation Workflow

1. **Pre-Execution**: Verify directory structure exists
2. **During Execution**: Capture real-time metrics and timestamps
3. **Post-Execution**: Generate complete report within 30 minutes
4. **Quality Control**: Verify all required sections are complete
5. **Archive**: Ensure report is saved in correct location with proper naming

**Report Validation Checklist:**

- [ ] **Header Section**: Complete with accurate metadata
- [ ] **Environment Configuration**: All system details captured
- [ ] **Individual Test Results**: All tests documented
- [ ] **Error Analysis**: All errors categorized and analyzed
- [ ] **Coverage Analysis**: Quality metrics calculated
- [ ] **Recommendations**: Actionable next steps provided
- [ ] **File Location**: Saved in `test_reports/` directory
- [ ] **Naming Convention**: Follows `Playwright_Tools_Test_Report_YY-MM-DD_hh-mm.md`

**Integration with Testing Protocols:**

- **Historical Tracking**: Reports archived for trend analysis and performance tracking

---

## ‚úÖ Implementation Checklists

### Playwright Tools Implementation Checklist

#### Pre-Execution Validation

- [ ] **Playwright Tools**: All playwright tools accessible and functional
- [ ] **Backend Health**: FastAPI server running on localhost:8000
- [ ] **Frontend Access**: React application running on localhost:3000
- [ ] **Browser Integration**: Playwright browser tools initialization successful

#### Test Execution Protocol

```
# Step 1: Initialize browser session
Use mcp__playwright__browser_navigate to start browser

# Step 2: Execute test sequence
For each test B001-B016:
  - Take snapshot via mcp__playwright__browser_snapshot
  - Execute test interactions via mcp__playwright__browser_click/type
  - Validate results and capture state
  - Proceed to next test in same session

# Step 3: Complete execution
Use mcp__playwright__browser_close to end session
```

#### Post-Execution Validation

- [ ] **Functional Success**: 100% functional validation achieved
- [ ] **Execution Time**: Within 43-57 second range (50s ¬±7s)
- [ ] **Session Continuity**: Single browser instance maintained
- [ ] **Playwright Operations**: Tool calls within expected range (150-200)
- [ ] **Error Handling**: Graceful recovery from any individual test failures

### Common Prerequisites

#### System Environment

- [ ] **Operating System**: Linux/WSL2 environment functional
- [ ] **Memory**: Sufficient RAM for browser automation (2GB+ recommended)

#### AI Agent Common Mistakes and Prevention

**CRITICAL**: The following mistakes have been identified in previous test reports and MUST be avoided:

1. **VERBATIM INPUT/OUTPUT ERRORS**:
   - ‚ùå **WRONG**: "Navigate to frontend and check model selector visibility"
   - ‚úÖ **CORRECT**: "Market Status: PRIORITY FAST REQUEST NEEDING QUICK RESPONSE..."
   - **PREVENTION**: Always capture the EXACT user input message, not test descriptions

2. **TEMPLATE FORMAT DEVIATIONS**:
   - ‚ùå **WRONG**: Using "Test X" instead of following template exactly
   - ‚úÖ **CORRECT**: Follow template structure precisely as specified
   - **PREVENTION**: Copy template format exactly without modifications

3. **NAMING CONVENTION ERRORS**:
   - ‚ùå **WRONG**: `Playwright_Tools_Test_Report_25-09-19_18-47.md` (single underscore)
   - ‚úÖ **CORRECT**: `Playwright_Tools_Test_Report__25-09-19_18-47.md` (double underscore)
   - **PREVENTION**: Use double underscore between "Report" and date

4. **OUTPUT FORMATTING ERRORS**:
   - ‚ùå **WRONG**: Showing JSON objects or test descriptions as output
   - ‚úÖ **CORRECT**: Showing complete AI response text verbatim
   - **PREVENTION**: Always capture the full AI response, not intermediate data

5. **TIMESTAMP DETECTION ERRORS**:
   - ‚ùå **WRONG**: Using training data cutoff dates
   - ‚úÖ **CORRECT**: Using `run_terminal_cmd` with `TZ='America/Los_Angeles'` commands
   - **PREVENTION**: Always execute the exact timestamp commands specified in template

- [ ] **Network**: Stable internet connection for API integrations
- [ ] **Permissions**: Appropriate file system access for test execution

#### Application Services

- [ ] **Backend Service**: FastAPI server operational on port 8000
- [ ] **Frontend Service**: React application operational on port 3000
- [ ] **API Keys**: OpenAI and Polygon API keys configured in environment
- [ ] **Database**: Application state management functional

#### Validation Services

- [ ] **Health Endpoints**: Both backend and frontend health checks passing
- [ ] **API Connectivity**: End-to-end API communication functional
- [ ] **Authentication**: Service authentication mechanisms operational
- [ ] **CORS Configuration**: Cross-origin requests properly configured

---

## üîß Troubleshooting Guide

### Common Issues and Solutions

#### Backend Server Issues

**Problem**: Backend health endpoint not responding

```bash
# Solution: Verify backend service
curl http://localhost:8000/health
# Expected: {"status":"ok"}

# If failing, restart backend
cd /home/1000211866/Github/market-parser-polygon-mcp/gpt5-openai-agents-sdk-polygon-mcp
uv run uvicorn src.main:app --host 0.0.0.0 --port 8000 --reload
```

**Problem**: API key configuration errors

```bash
# Solution: Verify environment variables
cat .env | grep -E "(POLYGON_API_KEY|OPENAI_API_KEY)"
# Both keys should be present and valid
```

#### Frontend Application Issues

**Problem**: Frontend not accessible

```bash
# Solution: Verify frontend service
curl http://localhost:3000/
# Should return HTML content

# If failing, restart frontend
cd /home/1000211866/Github/market-parser-polygon-mcp/gpt5-openai-agents-sdk-polygon-mcp/frontend_OpenAI
npm run dev
```

**Problem**: CORS errors in browser console

```bash
# Solution: Verify backend CORS configuration
# Check backend logs for CORS-related errors
# Ensure frontend origin is allowed in backend settings
```

#### Playwright Tools Testing Issues

**Problem**: Playwright tools not available

```bash
# Solution: Verify Playwright Tools environment
# Ensure all required Playwright tools are accessible:
# - mcp__playwright__browser_navigate
# - mcp__playwright__browser_snapshot
# - mcp__playwright__browser_click
# - mcp__playwright__browser_type
# - mcp__playwright__browser_close
```

**Problem**: Browser session management failures

```bash
# Solution: Implement proper session handling
# Ensure single browser session maintained throughout test execution
# Use proper error handling for session continuity
```

#### Performance Issues

**Problem**: Tests taking longer than expected

```bash
# Analysis steps:
# 1. Check system resource usage (CPU, memory)
# 2. Verify network connectivity stability
# 3. Monitor browser performance during execution
# 4. Compare against established baselines

# Playwright Tools baseline: 50s ¬±7s
```

**Problem**: Memory leaks during testing

```bash
# Solution: Monitor browser memory usage
# Ensure proper cleanup after test execution
# Check for JavaScript memory leaks in application
```

#### Integration Issues

**Problem**: API communication failures

```bash
# Solution: Test API endpoints individually
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "test"}'
# Should return valid JSON response
```

**Problem**: State management failures

```bash
# Solution: Verify application state handling
# Check browser console for state-related errors
# Ensure proper state persistence mechanisms
```

---

## üöÄ Optimization Recommendations

### Playwright Tools Optimizations

#### Current Performance Analysis

- **Execution Time**: 50s
- **Optimization Potential**: 10-15% improvement available
- **Tool Call Efficiency**: 150-200 operations can be optimized

#### Recommended Improvements

##### 1. Tool Call Optimization

```
Current: Individual tool calls for each action
Optimized: Batch operations where possible
Expected Improvement: 5-7% time reduction
```

##### 2. Browser Automation Efficiency

```
Current: Full snapshots for each validation
Optimized: Targeted element validation
Expected Improvement: 3-5% time reduction
```

##### 3. Parallel Processing Opportunities

```
Current: Sequential test execution
Optimized: Parallel execution for independent tests
Expected Improvement: 15-20% for suitable tests
```

##### 4. Session Management Optimization

```
Current: Full session state maintenance
Optimized: Minimal state tracking where appropriate
Expected Improvement: 2-3% time reduction
```

### Performance Monitoring Recommendations

#### Baseline Tracking

- **Implementation**: Capture execution timing for each test run
- **Alerting**: Flag performance degradation beyond tolerance ranges
- **Trending**: Monitor performance trends over time
- **Regression Detection**: Identify performance regressions early

#### Resource Optimization

- **Memory Management**: Monitor browser memory usage throughout execution
- **CPU Utilization**: Track system resource consumption
- **Network Efficiency**: Optimize API communication patterns
- **Storage**: Manage temporary files and test artifacts

#### Continuous Improvement

- **Regular Review**: Monthly performance baseline updates
- **Optimization Cycles**: Quarterly optimization implementation
- **Tool Updates**: Regular Playwright Tools and Playwright version updates
- **Best Practices**: Incorporate new optimization techniques as available

---

## üìû Support and Maintenance

### Documentation Maintenance

- **Review Schedule**: Monthly review and updates
- **Version Control**: Track changes and improvements
- **Community Feedback**: Incorporate user feedback and lessons learned
- **Best Practices**: Update with new testing methodologies and tools

### Performance Baseline Updates

- **Quarterly Reviews**: Update performance baselines based on system improvements
- **Environment Changes**: Adjust baselines for infrastructure modifications
- **Tool Updates**: Recalibrate for Playwright and Playwright Tools upgrades
- **Hardware Variations**: Account for different execution environments

### Contact Information

- **Primary Maintainer**: Documentation Specialist
- **Technical Lead**: Code Archaeologist
- **Performance Monitoring**: Backend Developer
- **Browser Automation**: React Component Architect

---

**End of Master Plan**

*This document serves as the definitive guide for Playwright testing of the Market Parser application using Playwright Tools methodology with established performance baselines and comprehensive error handling.*
